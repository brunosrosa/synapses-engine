#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de ValidaÃ§Ã£o Completa do Sistema RAG - Recoloca.ai

Este script executa uma validaÃ§Ã£o completa do sistema RAG incluindo:
- Teste de conectividade e performance
- ValidaÃ§Ã£o contextual especÃ­fica para @AgenteM_DevFastAPI
- Teste de qualidade das respostas
- VerificaÃ§Ã£o da infraestrutura

Autor: @AgenteM_DevFastAPI
VersÃ£o: 1.0
Data: Junho 2025
"""

import asyncio
import json
import logging
import sys
import time
from pathlib import Path
from typing import Dict, List, Any

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Adicionar core_logic ao path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

try:
    from rag_infra.core_logic.rag_retriever import RAGRetriever
    from rag_infra.core_logic.rag_indexer import RAGIndexer
    from rag_infra.core_logic.embedding_model import EmbeddingModelManager as EmbeddingModel
except ImportError as e:
    logger.error(f"Erro ao importar mÃ³dulos RAG: {e}")
    print(f"Erro detalhado: {e}")
    # NÃ£o usar sys.exit(1) em testes pytest
    raise ImportError(f"Falha ao importar mÃ³dulos RAG: {e}")

class RAGValidator:
    """Classe para validaÃ§Ã£o completa do sistema RAG"""
    
    def __init__(self):
        self.retriever = None
        self.indexer = None
        self.test_results = {
            "timestamp": time.time(),
            "tests": {},
            "overall_status": "pending",
            "recommendations": []
        }
    
    async def initialize_system(self) -> bool:
        """Inicializa o sistema RAG"""
        try:
            logger.info("ğŸ”„ Inicializando sistema RAG...")
            
            # Inicializar indexer
            self.indexer = RAGIndexer()
            
            # Inicializar retriever
            success = initialize_retriever(force_pytorch=True)
            if not success:
                raise Exception("Falha ao inicializar retriever")
            
            # Obter instÃ¢ncia do retriever
            self.retriever = get_retriever(force_pytorch=True)
            
            if self.retriever is None:
                raise Exception("Falha ao obter instÃ¢ncia do retriever")
            
            logger.info("âœ… Sistema RAG inicializado com sucesso")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erro na inicializaÃ§Ã£o: {e}")
            self.test_results["tests"]["initialization"] = {
                "status": "failed",
                "error": str(e)
            }
            return False
    
    async def test_connectivity_performance(self) -> Dict[str, Any]:
        """Testa conectividade e performance do sistema"""
        logger.info("ğŸ”„ Testando conectividade e performance...")
        
        test_result = {
            "status": "pending",
            "response_times": [],
            "queries_tested": 0,
            "average_response_time": 0,
            "errors": []
        }
        
        # Queries de teste para performance
        test_queries = [
            "FastAPI backend development",
            "Supabase integration",
            "MVP requirements",
            "authentication system",
            "API endpoints"
        ]
        
        try:
            for query in test_queries:
                start_time = time.time()
                
                try:
                    results = self.retriever.search(query, top_k=3)
                    response_time = time.time() - start_time
                    
                    test_result["response_times"].append(response_time)
                    test_result["queries_tested"] += 1
                    
                    logger.info(f"Query '{query}': {response_time:.3f}s - {len(results)} resultados")
                    
                except Exception as e:
                    test_result["errors"].append(f"Query '{query}': {str(e)}")
                    logger.error(f"Erro na query '{query}': {e}")
            
            # Calcular mÃ©tricas
            if test_result["response_times"]:
                test_result["average_response_time"] = sum(test_result["response_times"]) / len(test_result["response_times"])
                test_result["status"] = "success" if len(test_result["errors"]) == 0 else "partial"
            else:
                test_result["status"] = "failed"
            
            logger.info(f"âœ… Teste de performance concluÃ­do: {test_result['average_response_time']:.3f}s mÃ©dio")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"âŒ Erro no teste de performance: {e}")
        
        return test_result
    
    async def test_context_validation(self) -> Dict[str, Any]:
        """Valida contexto especÃ­fico para @AgenteM_DevFastAPI"""
        logger.info("ğŸ”„ Testando validaÃ§Ã£o contextual para @AgenteM_DevFastAPI...")
        
        test_result = {
            "status": "pending",
            "context_queries": [],
            "quality_score": 0,
            "relevant_documents_found": 0
        }
        
        # Queries especÃ­ficas para desenvolvimento FastAPI
        context_queries = [
            "FastAPI backend architecture Recoloca.ai",
            "Supabase authentication integration",
            "API endpoints specification",
            "Python development patterns",
            "MVP backend requirements"
        ]
        
        try:
            total_relevance = 0
            total_documents = 0
            
            for query in context_queries:
                try:
                    results = self.retriever.search(query, top_k=5)
                    
                    query_result = {
                        "query": query,
                        "results_count": len(results),
                        "top_documents": []
                    }
                    
                    for result in results[:3]:  # Top 3 resultados
                        # Converter SearchResult para dict se necessÃ¡rio
                        if hasattr(result, 'to_dict'):
                            result_dict = result.to_dict()
                        else:
                            result_dict = result
                        
                        query_result["top_documents"].append({
                            "document": result_dict.get("document", "Unknown"),
                            "score": result_dict.get("score", 0),
                            "content_preview": result_dict.get("content", "")[:100] + "..."
                        })
                    
                    test_result["context_queries"].append(query_result)
                    total_documents += len(results)
                    
                    # Calcular relevÃ¢ncia baseada na presenÃ§a de documentos
                    if len(results) > 0:
                        total_relevance += min(len(results), 5)  # Max 5 pontos por query
                    
                    logger.info(f"Query contextual '{query}': {len(results)} documentos encontrados")
                    
                except Exception as e:
                    logger.error(f"Erro na query contextual '{query}': {e}")
            
            # Calcular score de qualidade
            max_possible_score = len(context_queries) * 5
            test_result["quality_score"] = (total_relevance / max_possible_score) * 100 if max_possible_score > 0 else 0
            test_result["relevant_documents_found"] = total_documents
            
            if test_result["quality_score"] >= 70:
                test_result["status"] = "excellent"
            elif test_result["quality_score"] >= 50:
                test_result["status"] = "good"
            elif test_result["quality_score"] >= 30:
                test_result["status"] = "acceptable"
            else:
                test_result["status"] = "poor"
            
            logger.info(f"âœ… ValidaÃ§Ã£o contextual concluÃ­da: {test_result['quality_score']:.1f}% qualidade")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            logger.error(f"âŒ Erro na validaÃ§Ã£o contextual: {e}")
        
        return test_result
    
    async def test_mcp_integration(self) -> Dict[str, Any]:
        """Testa a integraÃ§Ã£o MCP"""
        logger.info("ğŸ”„ Testando integraÃ§Ã£o MCP...")
        
        test_result = {
            "status": "pending",
            "mcp_server_available": False,
            "config_valid": False
        }
        
        try:
            # Verificar se o arquivo de configuraÃ§Ã£o MCP existe
            mcp_config_path = Path(__file__).parent.parent / "config" / "trae_mcp_config.json"
            
            if mcp_config_path.exists():
                with open(mcp_config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    test_result["config_valid"] = True
                    test_result["config_content"] = config
                    logger.info("âœ… ConfiguraÃ§Ã£o MCP encontrada e vÃ¡lida")
            else:
                logger.warning("âš ï¸ Arquivo de configuraÃ§Ã£o MCP nÃ£o encontrado")
            
            # Verificar se o servidor MCP existe
            mcp_server_path = Path(__file__).parent / "mcp_server.py"
            
            if mcp_server_path.exists():
                test_result["mcp_server_available"] = True
                logger.info("âœ… Servidor MCP disponÃ­vel")
            else:
                logger.warning("âš ï¸ Servidor MCP nÃ£o encontrado")
            
            # Determinar status geral
            if test_result["config_valid"] and test_result["mcp_server_available"]:
                test_result["status"] = "ready"
            elif test_result["mcp_server_available"]:
                test_result["status"] = "partial"
            else:
                test_result["status"] = "not_ready"
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            logger.error(f"âŒ Erro no teste MCP: {e}")
        
        return test_result
    
    async def generate_recommendations(self) -> List[str]:
        """Gera recomendaÃ§Ãµes baseadas nos testes"""
        recommendations = []
        
        # Analisar resultados dos testes
        connectivity = self.test_results["tests"].get("connectivity_performance", {})
        context = self.test_results["tests"].get("context_validation", {})
        mcp = self.test_results["tests"].get("mcp_integration", {})
        
        # RecomendaÃ§Ãµes de performance
        if connectivity.get("average_response_time", 0) > 2.0:
            recommendations.append("ğŸ”´ [PERFORMANCE] Tempo de resposta alto. Considere otimizar o Ã­ndice ou usar cache.")
        elif connectivity.get("average_response_time", 0) < 0.5:
            recommendations.append("ğŸŸ¢ [PERFORMANCE] Excelente tempo de resposta. Sistema otimizado.")
        
        # RecomendaÃ§Ãµes de qualidade
        quality_score = context.get("quality_score", 0)
        if quality_score >= 70:
            recommendations.append("ğŸŸ¢ [QUALITY] Excelente qualidade contextual. RAG pronto para produÃ§Ã£o.")
        elif quality_score >= 50:
            recommendations.append("ğŸŸ¡ [QUALITY] Boa qualidade contextual. Considere adicionar mais documentos especÃ­ficos.")
        else:
            recommendations.append("ğŸ”´ [QUALITY] Qualidade contextual baixa. Re-indexaÃ§Ã£o ou revisÃ£o de documentos necessÃ¡ria.")
        
        # RecomendaÃ§Ãµes MCP
        mcp_status = mcp.get("status", "unknown")
        if mcp_status == "ready":
            recommendations.append("ğŸŸ¢ [MCP] Sistema MCP pronto para integraÃ§Ã£o com Trae IDE.")
        elif mcp_status == "partial":
            recommendations.append("ğŸŸ¡ [MCP] Sistema MCP parcialmente configurado. Verificar configuraÃ§Ã£o.")
        else:
            recommendations.append("ğŸ”´ [MCP] Sistema MCP nÃ£o configurado. ConfiguraÃ§Ã£o necessÃ¡ria para Trae IDE.")
        
        return recommendations
    
    async def run_full_validation(self) -> Dict[str, Any]:
        """Executa validaÃ§Ã£o completa do sistema RAG"""
        logger.info("ğŸš€ Iniciando validaÃ§Ã£o completa do sistema RAG...")
        
        # Inicializar sistema
        if not await self.initialize_system():
            self.test_results["overall_status"] = "failed"
            return self.test_results
        
        # Executar testes
        try:
            # Teste de conectividade e performance
            self.test_results["tests"]["connectivity_performance"] = await self.test_connectivity_performance()
            
            # Teste de validaÃ§Ã£o contextual
            self.test_results["tests"]["context_validation"] = await self.test_context_validation()
            
            # Teste de integraÃ§Ã£o MCP
            self.test_results["tests"]["mcp_integration"] = await self.test_mcp_integration()
            
            # Gerar recomendaÃ§Ãµes
            self.test_results["recommendations"] = await self.generate_recommendations()
            
            # Determinar status geral
            failed_tests = [test for test in self.test_results["tests"].values() if test.get("status") == "failed"]
            
            if len(failed_tests) == 0:
                self.test_results["overall_status"] = "success"
            elif len(failed_tests) < len(self.test_results["tests"]) / 2:
                self.test_results["overall_status"] = "partial"
            else:
                self.test_results["overall_status"] = "failed"
            
            logger.info(f"âœ… ValidaÃ§Ã£o completa concluÃ­da: {self.test_results['overall_status']}")
            
        except Exception as e:
            logger.error(f"âŒ Erro durante validaÃ§Ã£o: {e}")
            self.test_results["overall_status"] = "failed"
            self.test_results["error"] = str(e)
        
        return self.test_results
    
    def save_results(self, filename: str = "utils/rag_validation_results.json"):
        """Salva os resultados da validaÃ§Ã£o"""
        try:
            results_path = Path(__file__).parent / filename
            with open(results_path, 'w', encoding='utf-8') as f:
                json.dump(self.test_results, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“„ Resultados salvos em: {results_path}")
        except Exception as e:
            logger.error(f"âŒ Erro ao salvar resultados: {e}")
    
    def print_summary(self):
        """Imprime resumo dos resultados"""
        print("\n" + "="*60)
        print("ğŸ“‹ RESUMO DA VALIDAÃ‡ÃƒO RAG - RECOLOCA.AI")
        print("="*60)
        
        print(f"\nğŸ¯ STATUS GERAL: {self.test_results['overall_status'].upper()}")
        
        print("\nğŸ“Š RESULTADOS DOS TESTES:")
        for test_name, test_result in self.test_results["tests"].items():
            status_icon = {
                "success": "âœ…",
                "excellent": "ğŸŒŸ",
                "good": "âœ…",
                "acceptable": "ğŸŸ¡",
                "partial": "ğŸŸ¡",
                "poor": "ğŸ”´",
                "failed": "âŒ",
                "ready": "âœ…",
                "not_ready": "âŒ"
            }.get(test_result.get("status", "unknown"), "â“")
            
            print(f"  {status_icon} {test_name.replace('_', ' ').title()}: {test_result.get('status', 'unknown')}")
        
        print("\nğŸ’¡ RECOMENDAÃ‡Ã•ES:")
        for rec in self.test_results.get("recommendations", []):
            print(f"  {rec}")
        
        print("\n" + "="*60)

async def main():
    """FunÃ§Ã£o principal"""
    validator = RAGValidator()
    
    # Executar validaÃ§Ã£o completa
    results = await validator.run_full_validation()
    
    # Salvar resultados
    validator.save_results()
    
    # Imprimir resumo
    validator.print_summary()
    
    # Retornar cÃ³digo de saÃ­da baseado no status
    if results["overall_status"] == "success":
        print("\nğŸ‰ Sistema RAG totalmente operacional!")
        return 0
    elif results["overall_status"] == "partial":
        print("\nâš ï¸ Sistema RAG parcialmente operacional. Verificar recomendaÃ§Ãµes.")
        return 1
    else:
        print("\nâŒ Sistema RAG com problemas crÃ­ticos. CorreÃ§Ãµes necessÃ¡rias.")
        return 2

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)