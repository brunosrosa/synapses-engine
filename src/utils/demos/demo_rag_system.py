#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Demonstraﾃｧﾃ｣o Prﾃ｡tica do Sistema RAG Integrado
Para o projeto Recoloca.ai

Este script demonstra como usar o sistema RAG completo
com PyTorch GPU, otimizaﾃｧﾃｵes e cache persistente.

Autor: @AgenteM_DevFastAPI
Versﾃ｣o: 1.0
Data: Janeiro 2025
"""

import os
import sys
import time
import logging
from pathlib import Path
from typing import List, Dict, Any

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Adicionar path do projeto
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from rag_infra.src.core.core_logic.rag_retriever import RAGRetriever
from rag_infra.src.core.core_logic.pytorch_gpu_retriever import PyTorchGPURetriever
from rag_infra.src.core.core_logic.pytorch_optimizations import OptimizedPyTorchRetriever

class RAGSystemDemo:
    """Demonstraﾃｧﾃ｣o do sistema RAG completo."""
    
    def __init__(self):
        self.retrievers = {}
        self.demo_documents = [
            "Python ﾃｩ uma linguagem de programaﾃｧﾃ｣o de alto nﾃｭvel.",
            "FastAPI ﾃｩ um framework web moderno para Python.",
            "PyTorch ﾃｩ uma biblioteca de machine learning.",
            "CUDA permite computaﾃｧﾃ｣o paralela em GPUs NVIDIA.",
            "RAG combina recuperaﾃｧﾃ｣o de informaﾃｧﾃｵes com geraﾃｧﾃ｣o de texto.",
            "Supabase ﾃｩ uma alternativa open source ao Firebase.",
            "Recoloca.ai ﾃｩ uma plataforma de recolocaﾃｧﾃ｣o profissional.",
            "Machine Learning ajuda na anﾃ｡lise de currﾃｭculos.",
            "APIs REST facilitam a integraﾃｧﾃ｣o entre sistemas.",
            "Cache melhora a performance de aplicaﾃｧﾃｵes web."
        ]
    
    def demo_backend_detection(self):
        """Demonstra detecﾃｧﾃ｣o automﾃ｡tica de backend."""
        logger.info("[SEARCH] === DEMONSTRAﾃﾃグ: DETECﾃﾃグ DE BACKEND ===")
        
        retriever = RAGRetriever()
        backend_info = retriever.get_backend_info()
        
        logger.info("[EMOJI] Informaﾃｧﾃｵes do Backend:")
        for key, value in backend_info.items():
            logger.info(f"   {key}: {value}")
        
        return backend_info
    
    def demo_pytorch_gpu_retriever(self):
        """Demonstra uso do PyTorchGPURetriever."""
        logger.info("\n[START] === DEMONSTRAﾃﾃグ: PYTORCH GPU RETRIEVER ===")
        
        # Criar retriever
        retriever = PyTorchGPURetriever(batch_size=32)
        self.retrievers['pytorch'] = retriever
        
        # Inicializar
        logger.info("[EMOJI] Inicializando retriever...")
        init_success = retriever.initialize()
        logger.info(f"   Inicializaﾃｧﾃ｣o: {'[OK] Sucesso' if init_success else '[ERROR] Falhou'}")
        
        # Mostrar informaﾃｧﾃｵes
        index_info = retriever.get_index_info()
        logger.info("[EMOJI] Informaﾃｧﾃｵes do ﾃ肱dice:")
        for key, value in index_info.items():
            logger.info(f"   {key}: {value}")
        
        # Mostrar estatﾃｭsticas
        stats = retriever.get_stats()
        logger.info("[EMOJI] Estatﾃｭsticas:")
        logger.info(f"   GPU: {stats.get('gpu_name', 'N/A')}")
        logger.info(f"   Memﾃｳria GPU Alocada: {stats.get('gpu_memory_allocated', 0) / 1024**3:.2f} GB")
        logger.info(f"   Dispositivo: {stats.get('device', 'N/A')}")
        
        return retriever
    
    def demo_optimized_retriever(self):
        """Demonstra uso do OptimizedPyTorchRetriever."""
        logger.info("\n[EMOJI] === DEMONSTRAﾃﾃグ: OPTIMIZED PYTORCH RETRIEVER ===")
        
        # Criar retriever otimizado
        retriever = OptimizedPyTorchRetriever(
            cache_enabled=True,
            cache_ttl=300,  # 5 minutos
            batch_size=32,
            max_workers=4
        )
        self.retrievers['optimized'] = retriever
        
        # Inicializar
        logger.info("[EMOJI] Inicializando retriever otimizado...")
        init_success = retriever.initialize()
        logger.info(f"   Inicializaﾃｧﾃ｣o: {'[OK] Sucesso' if init_success else '[ERROR] Falhou'}")
        
        # Mostrar informaﾃｧﾃｵes do cache
        cache_info = retriever.get_cache_info()
        logger.info("[SAVE] Informaﾃｧﾃｵes do Cache:")
        for key, value in cache_info.items():
            logger.info(f"   {key}: {value}")
        
        # Mostrar estatﾃｭsticas completas
        stats = retriever.get_stats()
        logger.info("[EMOJI] Estatﾃｭsticas Completas:")
        logger.info(f"   Cache Habilitado: {stats.get('cache_enabled', False)}")
        logger.info(f"   Batch Size: {stats.get('batch_size', 'N/A')}")
        logger.info(f"   Max Workers: {stats.get('max_workers', 'N/A')}")
        
        return retriever
    
    def demo_rag_retriever_configurations(self):
        """Demonstra diferentes configuraﾃｧﾃｵes do RAGRetriever."""
        logger.info("\n[EMOJI] === DEMONSTRAﾃﾃグ: CONFIGURAﾃﾃ髭S RAG RETRIEVER ===")
        
        configurations = [
            {
                "name": "Auto (Padrﾃ｣o)",
                "params": {},
                "description": "Detecﾃｧﾃ｣o automﾃ｡tica do melhor backend"
            },
            {
                "name": "PyTorch Forﾃｧado",
                "params": {"force_pytorch": True},
                "description": "Forﾃｧa uso do PyTorch GPU"
            },
            {
                "name": "Otimizado com Cache",
                "params": {
                    "force_pytorch": True,
                    "use_optimizations": True,
                    "cache_enabled": True,
                    "batch_size": 64
                },
                "description": "PyTorch otimizado com cache persistente"
            }
        ]
        
        for config in configurations:
            logger.info(f"\n[EMOJI] Testando: {config['name']}")
            logger.info(f"   Descriﾃｧﾃ｣o: {config['description']}")
            
            try:
                retriever = RAGRetriever(**config['params'])
                backend_info = retriever.get_backend_info()
                index_info = retriever.get_index_info()
                
                logger.info(f"   [OK] Backend: {backend_info.get('current_backend', 'N/A')}")
                logger.info(f"   [EMOJI] Status: {index_info.get('status', 'N/A')}")
                
                self.retrievers[config['name']] = retriever
                
            except Exception as e:
                logger.error(f"   [ERROR] Erro: {e}")
    
    def demo_performance_comparison(self):
        """Demonstra comparaﾃｧﾃ｣o de performance entre backends."""
        logger.info("\n[EMOJI] === DEMONSTRAﾃﾃグ: COMPARAﾃﾃグ DE PERFORMANCE ===")
        
        import torch
        
        if not torch.cuda.is_available():
            logger.warning("[WARNING] GPU nﾃ｣o disponﾃｭvel. Pulando teste de performance.")
            return
        
        # Teste de operaﾃｧﾃｵes bﾃ｡sicas
        logger.info("ｧｪ Testando operaﾃｧﾃｵes bﾃ｡sicas...")
        
        # Teste 1: Multiplicaﾃｧﾃ｣o de matrizes
        sizes = [100, 500, 1000]
        
        for size in sizes:
            logger.info(f"\n[EMOJI] Testando matrizes {size}x{size}:")
            
            # CPU
            start_time = time.time()
            x_cpu = torch.randn(size, size)
            y_cpu = torch.randn(size, size)
            z_cpu = torch.mm(x_cpu, y_cpu)
            cpu_time = time.time() - start_time
            
            # GPU
            start_time = time.time()
            x_gpu = torch.randn(size, size, device='cuda')
            y_gpu = torch.randn(size, size, device='cuda')
            z_gpu = torch.mm(x_gpu, y_gpu)
            torch.cuda.synchronize()
            gpu_time = time.time() - start_time
            
            speedup = cpu_time / gpu_time if gpu_time > 0 else 0
            
            logger.info(f"   CPU: {cpu_time:.4f}s")
            logger.info(f"   GPU: {gpu_time:.4f}s")
            logger.info(f"   Speedup: {speedup:.2f}x")
    
    def demo_memory_usage(self):
        """Demonstra monitoramento de uso de memﾃｳria."""
        logger.info("\n[SAVE] === DEMONSTRAﾃﾃグ: MONITORAMENTO DE MEMﾃ迭IA ===")
        
        import torch
        
        if torch.cuda.is_available():
            # Memﾃｳria GPU
            gpu_memory_allocated = torch.cuda.memory_allocated(0)
            gpu_memory_reserved = torch.cuda.memory_reserved(0)
            gpu_memory_total = torch.cuda.get_device_properties(0).total_memory
            
            logger.info("[EMOJI] Memﾃｳria GPU:")
            logger.info(f"   Alocada: {gpu_memory_allocated / 1024**3:.2f} GB")
            logger.info(f"   Reservada: {gpu_memory_reserved / 1024**3:.2f} GB")
            logger.info(f"   Total: {gpu_memory_total / 1024**3:.2f} GB")
            logger.info(f"   Uso: {(gpu_memory_allocated / gpu_memory_total) * 100:.1f}%")
        
        # Memﾃｳria do sistema
        try:
            import psutil
            memory = psutil.virtual_memory()
            
            logger.info("\n[EMOJI] Memﾃｳria Sistema:")
            logger.info(f"   Total: {memory.total / 1024**3:.2f} GB")
            logger.info(f"   Disponﾃｭvel: {memory.available / 1024**3:.2f} GB")
            logger.info(f"   Uso: {memory.percent:.1f}%")
            
        except ImportError:
            logger.info("\n[WARNING] psutil nﾃ｣o disponﾃｭvel para monitoramento de memﾃｳria do sistema")
    
    def demo_best_practices(self):
        """Demonstra melhores prﾃ｡ticas de uso."""
        logger.info("\n[EMOJI] === DEMONSTRAﾃﾃグ: MELHORES PRﾃゝICAS ===")
        
        practices = [
            {
                "title": "1. Detecﾃｧﾃ｣o Automﾃ｡tica de Backend",
                "code": "retriever = RAGRetriever()  # Auto-detecta melhor backend",
                "benefit": "Adapta automaticamente ao hardware disponﾃｭvel"
            },
            {
                "title": "2. Forﾃｧar PyTorch para GPU",
                "code": "retriever = RAGRetriever(force_pytorch=True)",
                "benefit": "Garante uso de GPU quando disponﾃｭvel"
            },
            {
                "title": "3. Usar Otimizaﾃｧﾃｵes com Cache",
                "code": "retriever = RAGRetriever(use_optimizations=True, cache_enabled=True)",
                "benefit": "Mﾃ｡xima performance com cache persistente"
            },
            {
                "title": "4. Configurar Batch Size",
                "code": "retriever = RAGRetriever(batch_size=64)",
                "benefit": "Otimiza throughput para mﾃｺltiplas consultas"
            },
            {
                "title": "5. Monitorar Performance",
                "code": "stats = retriever.get_stats(); cache_info = retriever.get_cache_info()",
                "benefit": "Permite otimizaﾃｧﾃ｣o baseada em mﾃｩtricas reais"
            }
        ]
        
        for practice in practices:
            logger.info(f"\n[EMOJI] {practice['title']}")
            logger.info(f"   Cﾃｳdigo: {practice['code']}")
            logger.info(f"   Benefﾃｭcio: {practice['benefit']}")
    
    def run_complete_demo(self):
        """Executa demonstraﾃｧﾃ｣o completa do sistema."""
        logger.info("[EMOJI] INICIANDO DEMONSTRAﾃﾃグ COMPLETA DO SISTEMA RAG")
        logger.info("=" * 70)
        
        try:
            # 1. Detecﾃｧﾃ｣o de backend
            self.demo_backend_detection()
            
            # 2. PyTorch GPU Retriever
            self.demo_pytorch_gpu_retriever()
            
            # 3. Optimized Retriever
            self.demo_optimized_retriever()
            
            # 4. Configuraﾃｧﾃｵes RAG Retriever
            self.demo_rag_retriever_configurations()
            
            # 5. Comparaﾃｧﾃ｣o de performance
            self.demo_performance_comparison()
            
            # 6. Monitoramento de memﾃｳria
            self.demo_memory_usage()
            
            # 7. Melhores prﾃ｡ticas
            self.demo_best_practices()
            
            logger.info("\n" + "=" * 70)
            logger.info("[EMOJI] DEMONSTRAﾃﾃグ CONCLUﾃ好A COM SUCESSO!")
            logger.info("\n[EMOJI] RESUMO DAS FUNCIONALIDADES DEMONSTRADAS:")
            logger.info("   [OK] Detecﾃｧﾃ｣o automﾃ｡tica de backend")
            logger.info("   [OK] PyTorch GPU Retriever")
            logger.info("   [OK] Optimized Retriever com cache")
            logger.info("   [OK] Mﾃｺltiplas configuraﾃｧﾃｵes RAG")
            logger.info("   [OK] Comparaﾃｧﾃ｣o de performance")
            logger.info("   [OK] Monitoramento de memﾃｳria")
            logger.info("   [OK] Melhores prﾃ｡ticas de uso")
            
            logger.info("\n[START] O sistema RAG estﾃ｡ pronto para produﾃｧﾃ｣o!")
            
        except Exception as e:
            logger.error(f"[ERROR] Erro durante a demonstraﾃｧﾃ｣o: {e}")
            raise

def main():
    """Funﾃｧﾃ｣o principal."""
    demo = RAGSystemDemo()
    demo.run_complete_demo()

if __name__ == "__main__":
    main()