#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script para Reconstruir o Ãndice RAG

Este script corrige a inconsistÃªncia entre documentos (281) e metadados (7)
e reconstrÃ³i o Ã­ndice FAISS/PyTorch corretamente.

Autor: @AgenteM_DevFastAPI
VersÃ£o: 1.0
Data: Janeiro 2025
"""

import sys
import os
import json
import logging
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional
import shutil
from datetime import datetime

# Adicionar o diretÃ³rio pai ao path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'core_logic'))

from ..core_logic.constants import (
    FAISS_INDEX_DIR, FAISS_DOCUMENTS_FILE, FAISS_METADATA_FILE,
    EMBEDDING_MODEL_NAME, CHUNK_SIZE, CHUNK_OVERLAP
)

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class IndexRebuilder:
    """
    Classe para reconstruir o Ã­ndice RAG corrigindo inconsistÃªncias.
    """
    
    def __init__(self):
        self.backup_dir = FAISS_INDEX_DIR / "backup" / f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.source_docs_dir = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) / "source_documents"
        
    def rebuild_index(self) -> Dict[str, Any]:
        """
        ReconstrÃ³i o Ã­ndice completamente.
        
        Returns:
            Dict: RelatÃ³rio da reconstruÃ§Ã£o
        """
        logger.info("ğŸ”„ Iniciando reconstruÃ§Ã£o do Ã­ndice RAG...")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "backup_created": False,
            "inconsistency_fixed": False,
            "index_rebuilt": False,
            "documents_processed": 0,
            "chunks_created": 0,
            "embeddings_generated": 0,
            "final_status": {},
            "errors": []
        }
        
        try:
            # 1. Criar backup do Ã­ndice atual
            report["backup_created"] = self._create_backup()
            
            # 2. Analisar e corrigir inconsistÃªncia
            report["inconsistency_fixed"] = self._fix_inconsistency()
            
            # 3. Reconstruir Ã­ndice do zero
            rebuild_result = self._rebuild_from_source()
            report.update(rebuild_result)
            
            # 4. Verificar resultado final
            report["final_status"] = self._verify_index()
            
            # 5. Salvar relatÃ³rio
            self._save_report(report)
            
        except Exception as e:
            error_msg = f"Erro na reconstruÃ§Ã£o: {e}"
            report["errors"].append(error_msg)
            logger.error(f"âŒ {error_msg}")
        
        return report
    
    def _create_backup(self) -> bool:
        """
        Cria backup do Ã­ndice atual.
        """
        try:
            logger.info("ğŸ’¾ Criando backup do Ã­ndice atual...")
            
            if not FAISS_INDEX_DIR.exists():
                logger.warning("âš ï¸ DiretÃ³rio do Ã­ndice nÃ£o existe")
                return True
            
            # Criar diretÃ³rio de backup
            self.backup_dir.mkdir(parents=True, exist_ok=True)
            
            # Copiar arquivos importantes
            files_to_backup = [
                FAISS_DOCUMENTS_FILE,
                FAISS_METADATA_FILE,
                "embeddings.npy",
                "faiss_index.bin",
                "pytorch_conversion_info.json"
            ]
            
            backed_up = 0
            for filename in files_to_backup:
                source_path = FAISS_INDEX_DIR / filename
                if source_path.exists():
                    dest_path = self.backup_dir / filename
                    shutil.copy2(source_path, dest_path)
                    backed_up += 1
                    logger.info(f"ğŸ“ Backup: {filename}")
            
            logger.info(f"âœ… Backup criado: {backed_up} arquivos em {self.backup_dir}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erro ao criar backup: {e}")
            return False
    
    def _fix_inconsistency(self) -> bool:
        """
        Corrige a inconsistÃªncia entre documentos e metadados.
        """
        try:
            logger.info("ğŸ”§ Corrigindo inconsistÃªncia entre documentos e metadados...")
            
            documents_path = FAISS_INDEX_DIR / FAISS_DOCUMENTS_FILE
            metadata_path = FAISS_INDEX_DIR / FAISS_METADATA_FILE
            
            if not documents_path.exists() or not metadata_path.exists():
                logger.warning("âš ï¸ Arquivos de documentos ou metadados nÃ£o encontrados")
                return False
            
            # Carregar documentos e metadados
            with open(documents_path, 'r', encoding='utf-8') as f:
                documents = json.load(f)
            
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            logger.info(f"ğŸ“Š Documentos: {len(documents)}, Metadados: {len(metadata)}")
            
            if len(documents) == len(metadata):
                logger.info("âœ… Documentos e metadados jÃ¡ estÃ£o consistentes")
                return True
            
            # Corrigir inconsistÃªncia
            if len(documents) > len(metadata):
                logger.info("ğŸ”§ Gerando metadados faltantes...")
                
                # Gerar metadados para documentos sem metadados
                for i in range(len(metadata), len(documents)):
                    new_metadata = {
                        "source": f"documento_{i}",
                        "chunk_index": i,
                        "total_chunks": len(documents),
                        "file_path": "unknown",
                        "category": "general",
                        "created_at": datetime.now().isoformat()
                    }
                    metadata.append(new_metadata)
                
                # Salvar metadados corrigidos
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
                
                logger.info(f"âœ… Metadados corrigidos: {len(metadata)} entradas")
                
            elif len(metadata) > len(documents):
                logger.info("ğŸ”§ Removendo metadados extras...")
                
                # Truncar metadados para corresponder aos documentos
                metadata = metadata[:len(documents)]
                
                # Salvar metadados corrigidos
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
                
                logger.info(f"âœ… Metadados truncados: {len(metadata)} entradas")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Erro ao corrigir inconsistÃªncia: {e}")
            return False
    
    def _rebuild_from_source(self) -> Dict[str, Any]:
        """
        ReconstrÃ³i o Ã­ndice a partir dos documentos fonte.
        """
        result = {
            "index_rebuilt": False,
            "documents_processed": 0,
            "chunks_created": 0,
            "embeddings_generated": 0
        }
        
        try:
            logger.info("ğŸ—ï¸ Reconstruindo Ã­ndice a partir dos documentos fonte...")
            
            # Verificar se existe diretÃ³rio de documentos fonte
            if not self.source_docs_dir.exists():
                logger.error(f"âŒ DiretÃ³rio de documentos fonte nÃ£o encontrado: {self.source_docs_dir}")
                return result
            
            # Importar e usar o processador de documentos
            try:
                from ..core_logic.document_processor import DocumentProcessor
                from ..core_logic.embedding_generator import EmbeddingGenerator
                from ..core_logic.faiss_manager import FAISSManager
                
                # Inicializar componentes
                doc_processor = DocumentProcessor(
                    chunk_size=CHUNK_SIZE,
                    chunk_overlap=CHUNK_OVERLAP
                )
                
                embedding_generator = EmbeddingGenerator(
                    model_name=EMBEDDING_MODEL_NAME
                )
                
                faiss_manager = FAISSManager()
                
                # Processar documentos
                logger.info("ğŸ“„ Processando documentos...")
                
                all_chunks = []
                all_metadata = []
                
                # Encontrar todos os arquivos de documentos
                doc_files = list(self.source_docs_dir.rglob("*.md")) + list(self.source_docs_dir.rglob("*.txt"))
                
                for doc_file in doc_files:
                    try:
                        logger.info(f"ğŸ“– Processando: {doc_file.name}")
                        
                        # Ler conteÃºdo do arquivo
                        with open(doc_file, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # Processar em chunks
                        chunks = doc_processor.process_document(content, str(doc_file))
                        
                        # Adicionar chunks e metadados
                        for i, chunk in enumerate(chunks):
                            all_chunks.append(chunk["content"])
                            
                            metadata = {
                                "source": doc_file.name,
                                "chunk_index": i,
                                "total_chunks": len(chunks),
                                "file_path": str(doc_file.relative_to(self.source_docs_dir)),
                                "category": self._get_category_from_path(doc_file),
                                "created_at": datetime.now().isoformat()
                            }
                            all_metadata.append(metadata)
                        
                        result["documents_processed"] += 1
                        result["chunks_created"] += len(chunks)
                        
                    except Exception as e:
                        logger.error(f"âŒ Erro ao processar {doc_file}: {e}")
                        continue
                
                logger.info(f"ğŸ“Š Total: {len(all_chunks)} chunks de {result['documents_processed']} documentos")
                
                if not all_chunks:
                    logger.error("âŒ Nenhum chunk foi criado")
                    return result
                
                # Gerar embeddings
                logger.info("ğŸ§  Gerando embeddings...")
                embeddings = embedding_generator.generate_embeddings(all_chunks)
                result["embeddings_generated"] = len(embeddings)
                
                # Criar Ã­ndice FAISS
                logger.info("ğŸ” Criando Ã­ndice FAISS...")
                
                # Limpar diretÃ³rio do Ã­ndice
                if FAISS_INDEX_DIR.exists():
                    for file in FAISS_INDEX_DIR.glob("*"):
                        if file.is_file() and file.name != "backup":
                            file.unlink()
                
                FAISS_INDEX_DIR.mkdir(exist_ok=True)
                
                # Salvar documentos
                documents_path = FAISS_INDEX_DIR / FAISS_DOCUMENTS_FILE
                with open(documents_path, 'w', encoding='utf-8') as f:
                    json.dump(all_chunks, f, indent=2, ensure_ascii=False)
                
                # Salvar metadados
                metadata_path = FAISS_INDEX_DIR / FAISS_METADATA_FILE
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(all_metadata, f, indent=2, ensure_ascii=False)
                
                # Salvar embeddings
                embeddings_path = FAISS_INDEX_DIR / "embeddings.npy"
                np.save(embeddings_path, embeddings)
                
                # Criar Ã­ndice FAISS
                faiss_manager.create_index(embeddings)
                faiss_manager.save_index(FAISS_INDEX_DIR / "faiss_index.bin")
                
                result["index_rebuilt"] = True
                logger.info("âœ… Ãndice reconstruÃ­do com sucesso!")
                
            except ImportError as e:
                logger.error(f"âŒ Erro ao importar mÃ³dulos: {e}")
                logger.info("ğŸ”§ Tentando reconstruÃ§Ã£o simplificada...")
                result.update(self._simple_rebuild())
                
        except Exception as e:
            logger.error(f"âŒ Erro na reconstruÃ§Ã£o: {e}")
        
        return result
    
    def _simple_rebuild(self) -> Dict[str, Any]:
        """
        ReconstruÃ§Ã£o simplificada usando apenas os arquivos existentes.
        """
        result = {
            "index_rebuilt": False,
            "documents_processed": 0,
            "chunks_created": 0,
            "embeddings_generated": 0
        }
        
        try:
            logger.info("ğŸ”§ Executando reconstruÃ§Ã£o simplificada...")
            
            # Verificar se jÃ¡ temos documentos processados
            documents_path = FAISS_INDEX_DIR / FAISS_DOCUMENTS_FILE
            if documents_path.exists():
                with open(documents_path, 'r', encoding='utf-8') as f:
                    documents = json.load(f)
                
                # Recriar metadados consistentes
                metadata = []
                for i, doc in enumerate(documents):
                    metadata.append({
                        "source": f"chunk_{i}",
                        "chunk_index": i,
                        "total_chunks": len(documents),
                        "file_path": "processed",
                        "category": "general",
                        "created_at": datetime.now().isoformat()
                    })
                
                # Salvar metadados corrigidos
                metadata_path = FAISS_INDEX_DIR / FAISS_METADATA_FILE
                with open(metadata_path, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2, ensure_ascii=False)
                
                result["documents_processed"] = 1
                result["chunks_created"] = len(documents)
                result["index_rebuilt"] = True
                
                logger.info(f"âœ… ReconstruÃ§Ã£o simplificada: {len(documents)} chunks, {len(metadata)} metadados")
            
        except Exception as e:
            logger.error(f"âŒ Erro na reconstruÃ§Ã£o simplificada: {e}")
        
        return result
    
    def _get_category_from_path(self, file_path: Path) -> str:
        """
        Determina a categoria baseada no caminho do arquivo.
        """
        path_str = str(file_path).lower()
        
        if "arquitetura" in path_str:
            return "architecture"
        elif "requisitos" in path_str:
            return "requirements"
        elif "design" in path_str or "interface" in path_str:
            return "design"
        elif "api" in path_str:
            return "api"
        elif "tech_stack" in path_str or "tecnologia" in path_str:
            return "technology"
        else:
            return "general"
    
    def _verify_index(self) -> Dict[str, Any]:
        """
        Verifica se o Ã­ndice foi reconstruÃ­do corretamente.
        """
        status = {
            "documents_count": 0,
            "metadata_count": 0,
            "consistent": False,
            "files_present": [],
            "index_loadable": False
        }
        
        try:
            # Verificar arquivos
            required_files = [FAISS_DOCUMENTS_FILE, FAISS_METADATA_FILE]
            
            for filename in required_files:
                file_path = FAISS_INDEX_DIR / filename
                if file_path.exists():
                    status["files_present"].append(filename)
            
            # Contar documentos e metadados
            documents_path = FAISS_INDEX_DIR / FAISS_DOCUMENTS_FILE
            if documents_path.exists():
                with open(documents_path, 'r', encoding='utf-8') as f:
                    documents = json.load(f)
                    status["documents_count"] = len(documents)
            
            metadata_path = FAISS_INDEX_DIR / FAISS_METADATA_FILE
            if metadata_path.exists():
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    status["metadata_count"] = len(metadata)
            
            # Verificar consistÃªncia
            status["consistent"] = (status["documents_count"] == status["metadata_count"] and 
                                  status["documents_count"] > 0)
            
            # Testar carregamento do Ã­ndice
            try:
                from ..core_logic.rag_retriever import RAGRetriever
                retriever = RAGRetriever()
                if retriever.initialize() and retriever.load_index():
                    status["index_loadable"] = True
            except Exception as e:
                logger.warning(f"âš ï¸ Erro ao testar carregamento: {e}")
            
            logger.info(f"ğŸ“Š VerificaÃ§Ã£o final: {status['documents_count']} docs, {status['metadata_count']} meta, consistente: {status['consistent']}")
            
        except Exception as e:
            logger.error(f"âŒ Erro na verificaÃ§Ã£o: {e}")
        
        return status
    
    def _save_report(self, report: Dict[str, Any]):
        """
        Salva o relatÃ³rio de reconstruÃ§Ã£o.
        """
        try:
            # Importar configuraÃ§Ã£o centralizada
             import sys
             sys.path.append(str(Path(__file__).parent.parent))
             from config import get_report_path, REPORT_CONFIG
             
             report_path = get_report_path("index_rebuild_report.json")
             with open(report_path, 'w', encoding=REPORT_CONFIG['encoding']) as f:
                 json.dump(report, f, 
                          indent=REPORT_CONFIG['indent'], 
                          ensure_ascii=REPORT_CONFIG['ensure_ascii'], 
                          default=REPORT_CONFIG['default_serializer'])
             logger.info(f"ğŸ“„ RelatÃ³rio salvo em: {report_path.absolute()}")
        except Exception as e:
            logger.error(f"Erro ao salvar relatÃ³rio: {e}")

def main():
    """
    FunÃ§Ã£o principal do script de reconstruÃ§Ã£o.
    """
    print("ğŸ”„ Iniciando reconstruÃ§Ã£o do Ã­ndice RAG...")
    
    rebuilder = IndexRebuilder()
    report = rebuilder.rebuild_index()
    
    print("\n" + "="*80)
    print("ğŸ”„ RELATÃ“RIO DE RECONSTRUÃ‡ÃƒO DO ÃNDICE")
    print("="*80)
    
    print(f"\nğŸ“Š Resultados:")
    print(f"   Backup criado: {report['backup_created']}")
    print(f"   InconsistÃªncia corrigida: {report['inconsistency_fixed']}")
    print(f"   Ãndice reconstruÃ­do: {report['index_rebuilt']}")
    print(f"   Documentos processados: {report['documents_processed']}")
    print(f"   Chunks criados: {report['chunks_created']}")
    
    final_status = report.get('final_status', {})
    print(f"\nâœ… Status Final:")
    print(f"   Documentos: {final_status.get('documents_count', 0)}")
    print(f"   Metadados: {final_status.get('metadata_count', 0)}")
    print(f"   Consistente: {final_status.get('consistent', False)}")
    print(f"   CarregÃ¡vel: {final_status.get('index_loadable', False)}")
    
    if report.get('errors'):
        print(f"\nâŒ Erros ({len(report['errors'])}):")
        for i, error in enumerate(report['errors'], 1):
            print(f"   {i}. {error}")
    
    print("\n" + "="*80)
    
    if (final_status.get('consistent') and final_status.get('index_loadable')):
        print("ğŸ‰ ReconstruÃ§Ã£o bem-sucedida! Ãndice funcionando.")
    else:
        print("âš ï¸ ReconstruÃ§Ã£o parcial. Verifique os erros acima.")

if __name__ == "__main__":
    main()